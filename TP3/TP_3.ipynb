{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2547cf4f",
   "metadata": {},
   "source": [
    "# Trabajo Práctico Nº3 - Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c31dbb",
   "metadata": {},
   "source": [
    "### Mateo Servent, Joaquín Musich y Andres Cuellas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790e981",
   "metadata": {},
   "source": [
    "### Parte I: Análisis de la base de hogares y cálculo de pobreza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3314f287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANO4</th>\n",
       "      <td>16815.0</td>\n",
       "      <td>2023.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>2023.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRIMESTRE</th>\n",
       "      <td>16815.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NRO_HOGAR</th>\n",
       "      <td>16815.0</td>\n",
       "      <td>1.064347</td>\n",
       "      <td>1.177592</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REALIZADA</th>\n",
       "      <td>16815.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGION</th>\n",
       "      <td>16815.0</td>\n",
       "      <td>35.439191</td>\n",
       "      <td>15.237237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VII1_2</th>\n",
       "      <td>16815.0</td>\n",
       "      <td>0.590009</td>\n",
       "      <td>2.756843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VII2_1</th>\n",
       "      <td>16815.0</td>\n",
       "      <td>70.012608</td>\n",
       "      <td>43.469641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VII2_2</th>\n",
       "      <td>16815.0</td>\n",
       "      <td>0.403449</td>\n",
       "      <td>3.397350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VII2_3</th>\n",
       "      <td>16815.0</td>\n",
       "      <td>0.103836</td>\n",
       "      <td>1.289136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VII2_4</th>\n",
       "      <td>16815.0</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.174588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             count         mean        std     min     25%     50%     75%  \\\n",
       "ANO4       16815.0  2023.000000   0.000000  2023.0  2023.0  2023.0  2023.0   \n",
       "TRIMESTRE  16815.0     1.000000   0.000000     1.0     1.0     1.0     1.0   \n",
       "NRO_HOGAR  16815.0     1.064347   1.177592     1.0     1.0     1.0     1.0   \n",
       "REALIZADA  16815.0     1.000000   0.000000     1.0     1.0     1.0     1.0   \n",
       "REGION     16815.0    35.439191  15.237237     1.0    40.0    42.0    43.0   \n",
       "...            ...          ...        ...     ...     ...     ...     ...   \n",
       "VII1_2     16815.0     0.590009   2.756843     0.0     0.0     0.0     0.0   \n",
       "VII2_1     16815.0    70.012608  43.469641     1.0     3.0    98.0    98.0   \n",
       "VII2_2     16815.0     0.403449   3.397350     0.0     0.0     0.0     0.0   \n",
       "VII2_3     16815.0     0.103836   1.289136     0.0     0.0     0.0     0.0   \n",
       "VII2_4     16815.0     0.005412   0.174588     0.0     0.0     0.0     0.0   \n",
       "\n",
       "              max  \n",
       "ANO4       2023.0  \n",
       "TRIMESTRE     1.0  \n",
       "NRO_HOGAR    52.0  \n",
       "REALIZADA     1.0  \n",
       "REGION       44.0  \n",
       "...           ...  \n",
       "VII1_2       97.0  \n",
       "VII2_1       98.0  \n",
       "VII2_2       98.0  \n",
       "VII2_3       98.0  \n",
       "VII2_4        7.0  \n",
       "\n",
       "[81 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer la base de datos\n",
    "df = pd.read_excel('C:/Users/joaqu/Downloads/usu_hogar_T123.xlsx')\n",
    "\n",
    "\n",
    "# Estadísticos descriptivos (más la exploración del libro de codigos)\n",
    " \n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e75b73f",
   "metadata": {},
   "source": [
    "### A partir de un análisis preliminar, consideramos que posibles determinantes claves en la predicción de la pobreza dentro del conjunto de datos de hogares serían los atributos vinculados a la infraestructura habitacional (calidad estructural, conectividad a servicios básicos, instalaciones sanitarias), el esquema de propiedad o alquiler del inmueble y la distribución demográfica por edad en el núcleo familiar. Estos indicadores, podrían llegar a ser considerados esenciales para una estimación más afinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed613c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)\n",
    "# Filtrar observaciones\n",
    "df_filtrado = df[df['AGLOMERADO'].isin([32, 33])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95212098",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/macbook/Desktop/UdeSa/Materias/Big data/Big-Data/TP2/EPH_usu_1er_Trim_2023/usu_individual_T123.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m dfI \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/macbook/Desktop/UdeSa/Materias/Big data/Big-Data/TP2/EPH_usu_1er_Trim_2023/usu_individual_T123.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m dfI \u001b[38;5;241m=\u001b[39m dfI[dfI[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAGLOMERADO\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m33\u001b[39m])]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Definimos las intersección entre las dos bases como una lista\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(io, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, engine\u001b[38;5;241m=\u001b[39mengine)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1652\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1653\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1654\u001b[0m     )\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1656\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1523\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1526\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1527\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1528\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1529\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/macbook/Desktop/UdeSa/Materias/Big data/Big-Data/TP2/EPH_usu_1er_Trim_2023/usu_individual_T123.xlsx'"
     ]
    }
   ],
   "source": [
    "# 3)\n",
    "\n",
    "dfI = pd.read_excel('/Users/macbook/Desktop/UdeSa/Materias/Big data/Big-Data/TP2/EPH_usu_1er_Trim_2023/usu_individual_T123.xlsx')\n",
    "\n",
    "dfI = dfI[dfI['AGLOMERADO'].isin([32, 33])]\n",
    "\n",
    "# Definimos las intersección entre las dos bases como una lista\n",
    "columnas_duplicadas = set(dfI.columns).intersection(set(df_filtrado.columns))\n",
    "# Removemos CODUSU y NRO_HOGAR de la lista ya que queremos usarlas para la intersección \n",
    "#(y las columnas sobre las que se une no generan duplicados)\n",
    "columnas_duplicadas.remove(\"CODUSU\")\n",
    "columnas_duplicadas.remove(\"NRO_HOGAR\")\n",
    "\n",
    "# Hacemos el merge habiendo dropeado las columnas repetidas\n",
    "df2 = pd.merge(\n",
    "    dfI.drop(columnas_duplicadas, axis=1), \n",
    "    df_filtrado,\n",
    "    on= ['NRO_HOGAR', 'CODUSU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a199ef7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m         df \u001b[38;5;241m=\u001b[39m df[df[column] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m9\u001b[39m]\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 18\u001b[0m df3 \u001b[38;5;241m=\u001b[39m eliminar_categoria_9(df2, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNIVEL_ED\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCH08\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCH07\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCAT_OCUP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCH15\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCH16\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Hay más NAs en otras variables\u001b[39;00m\n\u001b[0;32m     20\u001b[0m df4 \u001b[38;5;241m=\u001b[39m df3[(df3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCH06\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "# 4 y 5 ) \n",
    "def eliminar_categoria_9(df, column_list):\n",
    "    \"\"\"\n",
    "    Elimina la categoría con valor 9 de una serie de variables en un DataFrame.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df: DataFrame donde se encuentran las columnas.\n",
    "    - column_list: Lista de columnas en las que se quiere eliminar la categoría con valor 9.\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame con las categorías con valor 9 eliminadas en las columnas especificadas.\n",
    "    \"\"\"\n",
    "    for column in column_list:\n",
    "        df = df[df[column] != 9]\n",
    "    return df\n",
    "\n",
    "\n",
    "df3 = eliminar_categoria_9(df2, ['NIVEL_ED', 'CH08', 'CH07', 'CAT_OCUP', 'CH15', 'CH16']) \n",
    "# Hay más NAs en otras variables\n",
    "df4 = df3[(df3['CH06'] > 0)] # Edad menor a cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b08781bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def columnas_duplicadas(df):\n",
    "    \"\"\"\n",
    "    Esta función verifica si hay columnas con valores idénticos en un DataFrame.\n",
    "    \n",
    "    Argumentos:\n",
    "    - df: DataFrame de pandas\n",
    "    \n",
    "    Retorna:\n",
    "    - Una lista de tuplas con los nombres de las columnas repetidas.\n",
    "    \"\"\"\n",
    "    \n",
    "    columnas_repetidas = []\n",
    "    \n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(i+1, df.shape[1]):\n",
    "            if df.iloc[:, i].equals(df.iloc[:, j]):\n",
    "                columnas_repetidas.append((df.columns[i], df.columns[j]))\n",
    "    \n",
    "    return columnas_repetidas\n",
    "\n",
    "\n",
    "def eliminar_columnas_duplicadas(df):\n",
    "    # Obtener las columnas duplicadas\n",
    "    cols_duplicadas = columnas_duplicadas(df)\n",
    "    \n",
    "    # Crear una lista de columnas para eliminar\n",
    "    columnas_para_eliminar = [col[1] for col in cols_duplicadas]\n",
    "    \n",
    "    # Eliminar las columnas\n",
    "    df = df.drop(columns=columnas_para_eliminar)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5688dc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m eliminar_columnas_duplicadas(df4) \u001b[38;5;66;03m# verificamos si hubiera columnas duplicadas y las eliminamos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop_high_na\u001b[39m(df, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Elimina las columnas y las filas del DataFrame que tienen un porcentaje de valores faltantes \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    superior al umbral especificado.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    - DataFrame después de eliminar las columnas y filas con un alto porcentaje de valores faltantes.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df4' is not defined"
     ]
    }
   ],
   "source": [
    "eliminar_columnas_duplicadas(df4) # verificamos si hubiera columnas duplicadas y las eliminamos\n",
    "\n",
    "def drop_high_na(df, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Elimina las columnas y las filas del DataFrame que tienen un porcentaje de valores faltantes \n",
    "    superior al umbral especificado.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame de entrada.\n",
    "    - threshold: umbral de porcentaje de valores faltantes para eliminar una columna o fila (valor predeterminado = 0.7).\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame después de eliminar las columnas y filas con un alto porcentaje de valores faltantes.\n",
    "    \"\"\"\n",
    "    # Calcula el porcentaje de valores faltantes para cada columna\n",
    "    missing_percentage_col = df.isnull().mean()\n",
    "    \n",
    "    # Filtra las columnas que tienen un porcentaje de valores faltantes superior al umbral\n",
    "    columns_to_drop = missing_percentage_col[missing_percentage_col > threshold].index\n",
    "    \n",
    "    # Elimina las columnas seleccionadas del DataFrame\n",
    "    df_cleaned = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Calcula el porcentaje de valores faltantes para cada fila\n",
    "    missing_percentage_row = df_cleaned.isnull().mean(axis=1)\n",
    "    \n",
    "    # Filtra las filas que tienen un porcentaje de valores faltantes superior al umbral\n",
    "    rows_to_drop = missing_percentage_row[missing_percentage_row > threshold].index\n",
    "    \n",
    "    # Elimina las filas seleccionadas del DataFrame\n",
    "    df_cleaned = df_cleaned.drop(index=rows_to_drop)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "drop_high_na(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a030d7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Crear una copia del DataFrame para evitar SettingWithCopyWarning\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df4_copy \u001b[38;5;241m=\u001b[39m df4\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Ajustamos la variable 'Nivel_ed'. \u001b[39;00m\n\u001b[0;32m      5\u001b[0m df4_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNIVEL_ED\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df4' is not defined"
     ]
    }
   ],
   "source": [
    "# Crear una copia del DataFrame para evitar SettingWithCopyWarning\n",
    "df4_copy = df4.copy()\n",
    "\n",
    "# Ajustamos la variable 'Nivel_ed'. \n",
    "df4_copy['NIVEL_ED'].replace(7, 0, inplace=True)\n",
    "\n",
    "# Convertimos las variables categóricas a formato de cadena\n",
    "category_list = [ 'CH08', 'CH09', 'T_VI', 'V10_M', 'IV1', 'IV3', 'IV4', 'IV6', 'IV7', 'IV9', 'IV10', 'II7', 'II8', 'II9', 'IV11']\n",
    "df_categories = df4_copy[category_list].astype(str)\n",
    "\n",
    "# Creamos variables dummy para las variables categóricas\n",
    "dummy_data = pd.get_dummies(df_categories, prefix=category_list)\n",
    "\n",
    "# Fusionamos las variables dummy con el DataFrame original, excluyendo las originales\n",
    "df4_copy = pd.concat([df4_copy, dummy_data], axis=1)\n",
    "df4_copy.drop(category_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ce06e5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df4_copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df4 \u001b[38;5;241m=\u001b[39m df4_copy\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Eliminamos las columnas ANO4 y TRIMESTRE de df4\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df4\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANO4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRIMESTRE\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df4_copy' is not defined"
     ]
    }
   ],
   "source": [
    "df4 = df4_copy.copy()\n",
    "\n",
    "# Eliminamos las columnas ANO4 y TRIMESTRE de df4\n",
    "df4.drop(['ANO4', 'TRIMESTRE'], axis=1, inplace=True)\n",
    "\n",
    "def resumen_dataframe(df):\n",
    "    print(f\"Total de filas: {df.shape[0]}\")\n",
    "    print(f\"Total de columnas: {df.shape[1]}\")\n",
    "\n",
    "resumen_dataframe(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9766e4db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 6) Estadísticas descriptivas\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# IV6: Suministro de agua. 1. por cañería dentro de la vivienda\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# IV3: Tipos de piso interior. 1. mosaico / baldosa / madera /cerámica / alfombra\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# IV11: Desague del baño. 1: a red pública (cloaca),  3. solo pozo ciego y 4. hoyo/excavación en la tierra\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[0;32m      9\u001b[0m variables \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIV3_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIV11_4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIV6_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIV11_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIV11_3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Espacio entre las barras\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# 6) Estadísticas descriptivas\n",
    "# IV6: Suministro de agua. 1. por cañería dentro de la vivienda\n",
    "# IV3: Tipos de piso interior. 1. mosaico / baldosa / madera /cerámica / alfombra\n",
    "# IV11: Desague del baño. 1: a red pública (cloaca),  3. solo pozo ciego y 4. hoyo/excavación en la tierra\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "variables = ['IV3_1', 'IV11_4', 'IV6_1', 'IV11_1', 'IV11_3']\n",
    "\n",
    "# Espacio entre las barras\n",
    "bar_width = 0.35\n",
    "index = range(len(variables))\n",
    "\n",
    "# Loop por cada variable\n",
    "bar1_values = []\n",
    "bar2_values = []\n",
    "for var in variables:\n",
    "    count_0 = (df4[var] == 0).sum()\n",
    "    count_1 = (df4[var] == 1).sum()\n",
    "    bar1_values.append(count_0)\n",
    "    bar2_values.append(count_1)\n",
    "\n",
    "bar1 = ax.bar(index, bar1_values, bar_width, label='0s', color='b')\n",
    "bar2 = ax.bar([i + bar_width for i in index], bar2_values, bar_width, label='1s', color='g')\n",
    "\n",
    "# Etiquetas y título\n",
    "ax.set_xlabel('Variables')\n",
    "ax.set_ylabel('Conteo')\n",
    "ax.set_title('Conteo de 0s y 1s para variables clave de hogar')\n",
    "ax.set_xticks([i + bar_width for i in index])\n",
    "ax.set_xticklabels(variables)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43496bbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/macbook/Desktop/UdeSa/Materias/Big data/Big-Data/TP2/tabla_adulto_equiv.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 7) \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m adultos_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/macbook/Desktop/UdeSa/Materias/Big data/Big-Data/TP2/tabla_adulto_equiv.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m adultos_data \u001b[38;5;241m=\u001b[39m adultos_data\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m adultos_data \u001b[38;5;241m=\u001b[39m adultos_data\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m23\u001b[39m, :]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(io, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, engine\u001b[38;5;241m=\u001b[39mengine)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1652\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1653\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1654\u001b[0m     )\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1656\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1523\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1526\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1527\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1528\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1529\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/macbook/Desktop/UdeSa/Materias/Big data/Big-Data/TP2/tabla_adulto_equiv.xlsx'"
     ]
    }
   ],
   "source": [
    "# 7) \n",
    "adultos_data = pd.read_excel(\"/Users/macbook/Desktop/UdeSa/Materias/Big data/Big-Data/TP2/tabla_adulto_equiv.xlsx\", skiprows=3)\n",
    "adultos_data = adultos_data.dropna(how='all')\n",
    "adultos_data = adultos_data.iloc[:23, :]\n",
    "\n",
    "def categorize_age(age):\n",
    "    if age == 0:\n",
    "        return \"Menor de 1 año\"\n",
    "    elif age == 1:\n",
    "        return \"1 año\"\n",
    "    elif age == 2:\n",
    "        return \"2 años\"\n",
    "    elif age == 3:\n",
    "        return \"3 años\"\n",
    "    elif age == 4:\n",
    "        return \"4 años\"\n",
    "    elif age == 5:\n",
    "        return \"5 años\"\n",
    "    elif age == 6:\n",
    "        return \"6 años\"\n",
    "    elif age == 7:\n",
    "        return \"7 años\"\n",
    "    elif age == 8:\n",
    "        return \"8 años\"\n",
    "    elif age == 9:\n",
    "        return \"9 años\"\n",
    "    elif age == 10:\n",
    "        return \"10 años\"\n",
    "    elif age == 11:\n",
    "        return \"11 años\"\n",
    "    elif age == 12:\n",
    "        return \"12 años\"\n",
    "    elif age == 13:\n",
    "        return \"13 años\"\n",
    "    elif age == 14:\n",
    "        return \"14 años\"\n",
    "    elif age == 15:\n",
    "        return \"15 años\"\n",
    "    elif age == 16:\n",
    "        return \"16 años\"\n",
    "    elif age == 17:\n",
    "        return \"17 años\"\n",
    "    elif 18 <= age <= 29:\n",
    "        return \"18 a 29 años\"\n",
    "    elif 30 <= age <= 45:\n",
    "        return \"30 a 45 años\"\n",
    "    elif 46 <= age <= 60:\n",
    "        return \"46 a 60 años\"\n",
    "    elif 61 <= age <= 75:\n",
    "        return \"61 a 75 años\"\n",
    "    else:\n",
    "        return \"más de 75 años\"\n",
    "\n",
    "# Creando la columna \"Rango de Edad\"\n",
    "df4.loc[:, 'Edad'] = df4['CH06'].apply(categorize_age)\n",
    "\n",
    "# Creando dos dataframes: uno para Mujeres y otro para Varones\n",
    "df_mujeres = adultos_data[['Edad', 'Mujeres']].copy()\n",
    "df_mujeres['CH04'] = 2\n",
    "df_mujeres.rename(columns={'Mujeres': 'adulto_equiv'}, inplace=True)\n",
    "\n",
    "df_varones = adultos_data[['Edad', 'Varones']].copy()\n",
    "df_varones['CH04'] = 1\n",
    "df_varones.rename(columns={'Varones': 'adulto_equiv'}, inplace=True)\n",
    "\n",
    "# Concatenando ambos dataframes\n",
    "adultos_equiv_data = pd.concat([df_mujeres, df_varones], axis=0)\n",
    "\n",
    "# Reordenando las columnas y ordenando el dataframe por Edad\n",
    "adultos_equiv_data = adultos_equiv_data[['Edad', 'CH04', 'adulto_equiv']]\n",
    "adultos_equiv_data = adultos_equiv_data.sort_values(by=['Edad', 'CH04']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "a797fdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NRO_HOGAR</th>\n",
       "      <th>CODUSU</th>\n",
       "      <th>adulto_equiv</th>\n",
       "      <th>ad_equiv_hogar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TQRMNORUYHLMSMCDEIJAH00719364</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TQRMNORUYHLMSMCDEIJAH00719364</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TQRMNOSRQHJNSOCDEIJAH00802640</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>TQRMNOSRQHJNSOCDEIJAH00802640</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>TQRMNOSRQHJNSOCDEIJAH00802640</td>\n",
       "      <td>0.69</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>TQRMNOSWQHJLQRCDEIJAH00796254</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>TQRMNOSWQHJLQRCDEIJAH00796254</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>TQRMNOPYVHLMRLCDEIJAH00719346</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>TQRMNOPYVHLMRLCDEIJAH00719346</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>TQRMNOPYVHLMRLCDEIJAH00719346</td>\n",
       "      <td>1.02</td>\n",
       "      <td>2.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NRO_HOGAR                         CODUSU  adulto_equiv  ad_equiv_hogar\n",
       "0          1  TQRMNORUYHLMSMCDEIJAH00719364          0.67            1.50\n",
       "1          1  TQRMNORUYHLMSMCDEIJAH00719364          0.83            1.50\n",
       "2          1  TQRMNOSRQHJNSOCDEIJAH00802640          1.00            2.45\n",
       "3          1  TQRMNOSRQHJNSOCDEIJAH00802640          0.76            2.45\n",
       "4          1  TQRMNOSRQHJNSOCDEIJAH00802640          0.69            2.45\n",
       "5          1  TQRMNOSWQHJLQRCDEIJAH00796254          1.00            2.00\n",
       "6          1  TQRMNOSWQHJLQRCDEIJAH00796254          1.00            2.00\n",
       "7          1  TQRMNOPYVHLMRLCDEIJAH00719346          1.00            2.78\n",
       "8          1  TQRMNOPYVHLMRLCDEIJAH00719346          0.76            2.78\n",
       "9          1  TQRMNOPYVHLMRLCDEIJAH00719346          1.02            2.78"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uniendo df4 con adultos_equiv_data usando las columnas \"Edad\" y \"CH04\"\n",
    "df5 = pd.merge(df4, adultos_equiv_data[['Edad', 'CH04', 'adulto_equiv']],\n",
    "               on=['Edad', 'CH04'], how='left')\n",
    "\n",
    "# Mostrando las primeras filas de la base resultante\n",
    "df5[['CH04', 'CH06', 'Edad', 'adulto_equiv']].head()\n",
    "\n",
    "# Agrupar por NRO_HOGAR y CODUSU y calcular la suma de adultos_equiv para cada combinación\n",
    "sum_adulto_equiv = df5.groupby(['NRO_HOGAR', 'CODUSU'])['adulto_equiv'].sum().reset_index(name='ad_equiv_hogar')\n",
    "\n",
    "# Fusionar con df5 para asignar los valores calculados a la columna ad_equiv_hogar\n",
    "df6 = pd.merge(df5, sum_adulto_equiv, on=['NRO_HOGAR', 'CODUSU'], how='left')\n",
    "df6 = df6.copy()\n",
    "\n",
    "# Mostrar las primeras 10 filas del DataFrame para verificar\n",
    "df6[['NRO_HOGAR', 'CODUSU', 'adulto_equiv', 'ad_equiv_hogar']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "cee52294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) División entre quienes reportaron ingresos y entre quienes no lo hicieron\n",
    "# Ingreso neesario según canasta básica\n",
    "\n",
    "norespondieron = df6[(df6['DECIFR'] == 12) | (df6['DECIFR'] == 13) & (df6['ITF'] == 0)].copy()\n",
    "respondieron = df6[(df6['DECIFR'] != 12) & (df6['DECIFR'] != 13) & (df6['ITF'] != 0)].copy()\n",
    "\n",
    "canasta_basica_total = 57371.05\n",
    "\n",
    "respondieron['ingreso_necesario'] = respondieron['ad_equiv_hogar'] * canasta_basica_total\n",
    "norespondieron['ingreso_necesario'] = norespondieron['ad_equiv_hogar'] * canasta_basica_total\n",
    "\n",
    "#9) \n",
    "# Variable de conteo casos divididos por la línea de pobreza\n",
    "\n",
    "respondieron['pobre'] = (respondieron['ITF'] < respondieron['ingreso_necesario']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "fc8395e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) \n",
    "# Filtrar solo los hogares que son pobres\n",
    "hogares_pobres = respondieron[respondieron['pobre'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "da471c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número ponderado de hogares identificados como pobres: 1499962\n",
      "Porcentaje ponderado de hogares identificados como pobres: 28.24%\n"
     ]
    }
   ],
   "source": [
    "# Sumar el ponderador PONDIH para los hogares únicos\n",
    "total_hogares_ponderados = respondieron.drop_duplicates(subset=['CODUSU', 'NRO_HOGAR'])['PONDIH'].sum()\n",
    "\n",
    "# Sumar el ponderador PONDIH para los hogares únicos que son pobres\n",
    "hogares_pobres_ponderados = hogares_pobres.drop_duplicates(subset=['CODUSU', 'NRO_HOGAR'])['PONDIH'].sum()\n",
    "\n",
    "# Calcular el porcentaje de hogares pobres ponderados en relación al total ponderado\n",
    "porcentaje_hogares_pobres_ponderados = (hogares_pobres_ponderados / total_hogares_ponderados) * 100\n",
    "\n",
    "print(f'Número ponderado de hogares identificados como pobres: {hogares_pobres_ponderados}')\n",
    "print(f'Porcentaje ponderado de hogares identificados como pobres: {porcentaje_hogares_pobres_ponderados:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df9463",
   "metadata": {},
   "source": [
    "### La diferencia con los valores que reporta INDEC (30,3%), tiene que ver con el tratamiento que dieron a valores faltantes y al no reporte de ingresos. Recordamos que en nuestro caso eliminamos casos donde el 70% de los valores de la fila o columna eran NAs y, además, solo estamos considerando quienes reportaron ingresos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6733248",
   "metadata": {},
   "source": [
    "# Parte II: Construcción de funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "1be58ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def evalua_modelo(x_train, y_train, x_test, y_test, tipos=['logistico', 'LDA', 'kNN'], lambda_=1.0, k_neighbors=5):\n",
    "    # Estandarizar datos\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.transform(x_test)\n",
    "\n",
    "    # DataFrame para almacenar resultados\n",
    "    resultados = pd.DataFrame({\n",
    "        'Metrica': ['AUC', 'Accuracy', 'ECM', 'True P', 'False P', 'True N', 'False N'],\n",
    "        'Lambda': [np.nan] * 7,  # Columna vacía para lambda\n",
    "        'k_neighbors': [np.nan] * 7  # Columna vacía para k_neighbors\n",
    "    })\n",
    "\n",
    "    for tipo in tipos:\n",
    "        # Ajustar el modelo y establecer el hiperparámetro\n",
    "        if tipo == 'logistico':\n",
    "            modelo = LogisticRegression(C=1/lambda_, max_iter=10000).fit(x_train, y_train)\n",
    "            resultados['Lambda'] = lambda_\n",
    "        elif tipo == 'LDA':\n",
    "            modelo = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto').fit(x_train, y_train)\n",
    "        elif tipo == 'kNN':\n",
    "            modelo = KNeighborsClassifier(n_neighbors=k_neighbors).fit(x_train, y_train)\n",
    "            resultados['k_neighbors'] = k_neighbors\n",
    "        else:\n",
    "            raise ValueError(f\"Tipo de modelo {tipo} no reconocido.\")\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = modelo.predict(x_test)\n",
    "        y_pred_proba = modelo.predict_proba(x_test)[:, 1]\n",
    "        \n",
    "        # Métricas\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        ecm = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        # Almacenar resultados en el DataFrame\n",
    "        resultados[f'Valor_{tipo}'] = [auc, acc, ecm, \n",
    "                                      conf_matrix[1, 1], conf_matrix[0, 1],\n",
    "                                      conf_matrix[0, 0], conf_matrix[1, 0]]\n",
    "\n",
    "    # Redondear los valores a dos decimales\n",
    "    for col in resultados.columns[1:]:\n",
    "        resultados[col] = resultados[col].apply(lambda x: round(x, 3) if isinstance(x, (int, float)) else x)\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "5ab033da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de entrenamiento: X_train = 2919 Y_train = 2919\n",
      "Conjunto de prueba: X_test = 1252 Y_test = 1252\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrica</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>k_neighbors</th>\n",
       "      <th>Valor_logistico</th>\n",
       "      <th>Valor_LDA</th>\n",
       "      <th>Valor_kNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ECM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True P</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>418.000</td>\n",
       "      <td>416.000</td>\n",
       "      <td>321.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False P</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>21.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>80.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>785.000</td>\n",
       "      <td>751.000</td>\n",
       "      <td>726.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>28.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>125.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Metrica  Lambda  k_neighbors  Valor_logistico  Valor_LDA  Valor_kNN\n",
       "0       AUC     1.0            5            0.994      0.982      0.904\n",
       "1  Accuracy     1.0            5            0.961      0.932      0.836\n",
       "2       ECM     1.0            5            0.039      0.068      0.164\n",
       "3    True P     1.0            5          418.000    416.000    321.000\n",
       "4   False P     1.0            5           21.000     55.000     80.000\n",
       "5    True N     1.0            5          785.000    751.000    726.000\n",
       "6   False N     1.0            5           28.000     30.000    125.000"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separar las características (X) y la variable dependiente (y)\n",
    "X = respondieron.drop('pobre', axis=1)\n",
    "X = X.select_dtypes(include=[\"number\"])  # mantenemos las variables numéricas\n",
    "\n",
    "# Eliminar las columnas que solo contienen NaN\n",
    "X = X.dropna(axis=1, how='all')\n",
    "\n",
    "# Imputar datos faltantes con la media, es una medida conroversial. Permite continuar con las predicciones sin \n",
    "# perder mucha información, pero se atribuye valores incorrectos a casos que no reportan información.\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "# Asegurarse de que y tenga las mismas filas que X_imputed\n",
    "y_cleaned = respondieron.loc[X_imputed.index, 'pobre']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_imputed, y_cleaned, test_size=0.3, random_state=2023)\n",
    "\n",
    "\n",
    "print(\"Conjunto de entrenamiento:\", \"X_train =\", len(x_train), \"Y_train =\", len(y_train))\n",
    "print(\"Conjunto de prueba:\", \"X_test =\", len(x_test), \"Y_test =\", len(y_test))\n",
    "\n",
    "evalua_modelo(x_train, y_train, x_test, y_test) # evalua los 3 pero se podría seleccionar solo 1 o 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "87318fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Metrica</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>k_neighbors</th>\n",
       "      <th>Valor_logistico</th>\n",
       "      <th>Valor_LDA</th>\n",
       "      <th>Valor_kNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AUC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ECM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>True P</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>152.000</td>\n",
       "      <td>152.000</td>\n",
       "      <td>122.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>False P</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>26.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>10</td>\n",
       "      <td>ECM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>10</td>\n",
       "      <td>True P</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>150.000</td>\n",
       "      <td>147.000</td>\n",
       "      <td>119.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>10</td>\n",
       "      <td>False P</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>10</td>\n",
       "      <td>True N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>257.000</td>\n",
       "      <td>245.000</td>\n",
       "      <td>242.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>10</td>\n",
       "      <td>False N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>36.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fold   Metrica  Lambda  k_neighbors  Valor_logistico  Valor_LDA  Valor_kNN\n",
       "0      1       AUC     1.0            5            0.997      0.989      0.925\n",
       "1      1  Accuracy     1.0            5            0.978      0.959      0.859\n",
       "2      1       ECM     1.0            5            0.022      0.041      0.141\n",
       "3      1    True P     1.0            5          152.000    152.000    122.000\n",
       "4      1   False P     1.0            5            6.000     14.000     26.000\n",
       "..   ...       ...     ...          ...              ...        ...        ...\n",
       "65    10       ECM     1.0            5            0.024      0.060      0.134\n",
       "66    10    True P     1.0            5          150.000    147.000    119.000\n",
       "67    10   False P     1.0            5            5.000     17.000     20.000\n",
       "68    10    True N     1.0            5          257.000    245.000    242.000\n",
       "69    10   False N     1.0            5            5.000      8.000     36.000\n",
       "\n",
       "[70 rows x 7 columns]"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def cross_validation(X, y, k=10, modelos=['logistico', 'LDA', 'kNN']):\n",
    "    # El valor predeterminado para k particiones es 10\n",
    "    # modelos es una lista de los tipos de modelos a evaluar, por defecto se evalúan los tres.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2023) \n",
    "    # subconjuntos estratificados mejoran las particiones y dan mayor estabilidad a las métricas\n",
    "    resultados_totales = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):  # pasamos 'y' también a skf.split\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Evaluamos todos los modelos especificados en la lista modelos\n",
    "        resultados = evalua_modelo(X_train, y_train, X_test, y_test, modelos)\n",
    "        resultados_totales.append(resultados)\n",
    "\n",
    "    # Consolidar los resultados en un único DataFrame\n",
    "    resultados_df = pd.concat(resultados_totales, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Añadir una columna para identificar el fold\n",
    "    resultados_df['Fold'] = [i+1 for i in range(k) for _ in range(len(resultados))]\n",
    "    \n",
    "    # Reordenar las columnas para que 'Fold' aparezca al principio\n",
    "    cols = resultados_df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    resultados_df = resultados_df[cols]\n",
    "\n",
    "    return resultados_df\n",
    "\n",
    "cross_validation(X_imputed, y_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "3ae5a0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Optimo lambda para regresión logística': 1.0,\n",
       " 'Optima cantidad de particiones (fold) para LDA': 1,\n",
       " 'Optima cantidad de vecinos para kNN': 3.0}"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Integramos la dos funciones pasadas \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def evalua_config(X, y, lambda_values=[1.0], k_neighbors_values=[5], k=10, modelos=['logistico', 'LDA', 'kNN']):\n",
    "    # Lista para almacenar los resúmenes de todas las combinaciones\n",
    "    resumenes_totales = []\n",
    "    \n",
    "    # Iterar sobre cada valor de lambda y k_neighbors\n",
    "    for lambda_ in lambda_values:\n",
    "        for k_neighbors in k_neighbors_values:\n",
    "            \n",
    "            # Realizar la validación cruzada\n",
    "            resultados_df = cross_validation(X, y, k=k, modelos=modelos)\n",
    "            \n",
    "            # Evaluar el ECM mínimo y extraer la información correspondiente\n",
    "            metodos = ['Valor_logistico', 'Valor_LDA', 'Valor_kNN']\n",
    "            \n",
    "            for metodo in metodos:\n",
    "                min_ecm = resultados_df[resultados_df['Metrica'] == 'ECM'][metodo].min()\n",
    "                row = resultados_df[(resultados_df['Metrica'] == 'ECM') & (resultados_df[metodo] == min_ecm)]\n",
    "                \n",
    "                resumenes_totales.append({\n",
    "                    'Metodo': metodo,\n",
    "                    'ECM_minimo': min_ecm,\n",
    "                    'Lambda': lambda_ if 'logistico' in metodo else np.nan,\n",
    "                    'Fold': row['Fold'].values[0],\n",
    "                    'k_neighbors': k_neighbors if 'kNN' in metodo else np.nan\n",
    "                })\n",
    "\n",
    "    resumen_total = pd.DataFrame(resumenes_totales)\n",
    "    \n",
    "    # Encontrar los mejores parámetros para cada método\n",
    "    mejor_lambda_logistico = resumen_total[resumen_total['Metodo'] == 'Valor_logistico']['Lambda'].iloc[0]\n",
    "    mejor_fold_LDA = resumen_total[resumen_total['Metodo'] == 'Valor_LDA']['Fold'].iloc[0]\n",
    "    mejor_k_neighbors_kNN = resumen_total[resumen_total['Metodo'] == 'Valor_kNN']['k_neighbors'].iloc[0]\n",
    "    \n",
    "    return {\n",
    "        'Optimo lambda para regresión logística': mejor_lambda_logistico,\n",
    "        'Optima cantidad de particiones (fold) para LDA': mejor_fold_LDA,\n",
    "        'Optima cantidad de vecinos para kNN': mejor_k_neighbors_kNN\n",
    "    }\n",
    "\n",
    "# Ejemplo de uso:\n",
    "evalua_config(X_imputed, y_cleaned, lambda_values=[1, 10], k_neighbors_values=[3, 5], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "c95c5c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metodo</th>\n",
       "      <th>ECM_minimo</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>Fold</th>\n",
       "      <th>k_neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Valor_logistico</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Valor_LDA</td>\n",
       "      <td>0.032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Valor_kNN</td>\n",
       "      <td>0.108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Metodo  ECM_minimo  Lambda  Fold  k_neighbors\n",
       "0  Valor_logistico       0.007     1.0    14          NaN\n",
       "1        Valor_LDA       0.032     NaN    14          NaN\n",
       "2        Valor_kNN       0.108     NaN     2          3.0"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4)\n",
    "\n",
    "def evalua_multiples_metodos(X, y, lambda_values=[1.0], k_neighbors_values=[5], k=10, modelos=['logistico', 'LDA', 'kNN']):\n",
    "    # Lista para almacenar los resúmenes de todas las combinaciones\n",
    "    resumenes_totales = []\n",
    "    \n",
    "    # Iterar sobre cada valor de lambda y k_neighbors\n",
    "    for lambda_ in lambda_values:\n",
    "        for k_neighbors in k_neighbors_values:\n",
    "            \n",
    "            # Realizar la validación cruzada\n",
    "            resultados_df = cross_validation(X, y, k=k, modelos=modelos)\n",
    "            \n",
    "            # Evaluar el ECM mínimo y extraer la información correspondiente\n",
    "            metodos = ['Valor_logistico', 'Valor_LDA', 'Valor_kNN']\n",
    "            \n",
    "            for metodo in metodos:\n",
    "                min_ecm = resultados_df[resultados_df['Metrica'] == 'ECM'][metodo].min()\n",
    "                rows_with_min_ecm = resultados_df[(resultados_df['Metrica'] == 'ECM') & (resultados_df[metodo] == min_ecm)]\n",
    "                \n",
    "                # Seleccionar aleatoriamente una fila si hay múltiples filas con el mismo ECM mínimo para el método\n",
    "                selected_row = rows_with_min_ecm.sample(1).iloc[0]\n",
    "                \n",
    "                resumenes_totales.append({\n",
    "                    'Metodo': metodo,\n",
    "                    'ECM_minimo': min_ecm,\n",
    "                    'Lambda': lambda_ if 'logistico' in metodo else np.nan,\n",
    "                    'Fold': selected_row['Fold'],\n",
    "                    'k_neighbors': k_neighbors if 'kNN' in metodo else np.nan\n",
    "                })\n",
    "\n",
    "    resumen_total = pd.DataFrame(resumenes_totales)\n",
    "    resumen_total = resumen_total.drop_duplicates(subset=['ECM_minimo', 'Metodo']).reset_index(drop=True)\n",
    "    return resumen_total.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "evalua_multiples_metodos(X_imputed, y_cleaned, lambda_values=[1, 10], k_neighbors_values=[3, 5], k=15)\n",
    "# Probamos con otra cantidad de particiones, se reduce el ECM pero se corre más riesgo de encontrar más overfiting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2fea5",
   "metadata": {},
   "source": [
    "# Parte III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "5e9cd5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en X:\n",
      "Index(['NRO_HOGAR', 'COMPONENTE', 'H15', 'CH03', 'CH04', 'CH06', 'CH07',\n",
      "       'CH10', 'CH11', 'CH12',\n",
      "       ...\n",
      "       'II9_1', 'II9_2', 'II9_3', 'II9_4', 'IV11_0', 'IV11_1', 'IV11_2',\n",
      "       'IV11_3', 'IV11_4', 'constante'],\n",
      "      dtype='object', length=513)\n",
      "\n",
      "Columnas en Y:\n",
      "pobre\n"
     ]
    }
   ],
   "source": [
    "# 1)\n",
    "\n",
    "# Lista de columnas a eliminar\n",
    "columnas_eliminar = [\n",
    "    \"PP08D1\", \"PP08D4\", \"PP08F1\", \"PP08F2\", \"PP08J1\", \"PP08J2\", \"PP08J3\",\n",
    "    \"P21\", \"DECOCUR\", \"IDECOCUR\", \"RDECOCUR\", \"GDECOCUR\", \"PDECOCUR\", \"ADECOCUR\", \"PONDIIO\",\n",
    "    \"TOT_P12\",\n",
    "    \"P47T\", \"DECINDR\", \"IDECINDR\", \"RDECINDR\", \"GDECINDR\", \"PDECINDR\", \"ADECINDR\", \"PONDII\",\n",
    "    \"V2_M\", \"V3_M\", \"V4_M\", \"V5_M\", \"V8_M\", \"V9_M\", \"V11_M\", \"V12_M\", \"V18_M\", \"V19_AM\", \"V21_M\",\n",
    "    \"ITF\", \"DECIFR\", \"IDECIFR\", \"RDECIFR\", \"GDECIFR\", \"PDECIFR\", \"ADECIFR\",\n",
    "    \"IPCF\", \"DECCFR\", \"IDECCFR\", \"RDECCFR\", \"GDECCFR\", \"PDECCFR\", \"ADECCFR\", \"PONDIH\",\n",
    "    \"adulto_equiv\", \"ad_equiv_hogar\", \"ingreso_necesario\"]\n",
    "\n",
    "# Limpiando los dataframes\n",
    "clean_respondieron = respondieron.drop(columns=columnas_eliminar)\n",
    "clean_no_respondieron = norespondieron.drop(columns=columnas_eliminar)\n",
    "\n",
    "# Seleccionar todas las características observadas, excluyendo la columna \"pobre\"\n",
    "X = clean_respondieron.drop(columns=[\"pobre\"])\n",
    "X = X.select_dtypes(include=[\"number\"])  # mantenemos las variables numéricas\n",
    "# Eliminar las columnas que solo contienen NaN\n",
    "X = X.dropna(axis=1, how='all')\n",
    "\n",
    "# Añadir una columna constante al conjunto de características observables\n",
    "X[\"constante\"] = 1\n",
    "\n",
    "# Imputar datos faltantes con la media, es una medida conroversial. Permite continuar con las predicciones sin \n",
    "# perder mucha información, pero se atribuye valores incorrectos a casos que no reportan información.\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "# Establecer la variable dependiente con las mismas filas que X_imputed\n",
    "y_cleaned = respondieron.loc[X_imputed.index, 'pobre']\n",
    "\n",
    "# Imprimiendo las columnas restantes de cada dataframe\n",
    "print(\"Columnas en X:\")\n",
    "print(X_imputed.columns)\n",
    "\n",
    "print(\"\\nColumnas en Y:\")\n",
    "print(y_cleaned.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "ced32c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodo        Valor_logistico\n",
      "ECM_minimo              0.144\n",
      "Lambda                    1.0\n",
      "Fold                        1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2) \n",
    "\n",
    "evaluacion = evalua_multiples_metodos(X_imputed, y_cleaned, lambda_values=[1, 10])\n",
    "\n",
    "print(evaluacion.iloc[0, 0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ff53e",
   "metadata": {},
   "source": [
    "### 3) La elección del parámetro de regularización, λ, es crucial para prevenir el sobreajuste en modelos como la regresión Lasso o Ridge. Utilizamos validación cruzada, como el método k-fold, para determinar el λ óptimo en el conjunto de entrenamiento. Es necesario no usar el conjunto de prueba en este proceso para evitar el \"data leakage\", que comprometería la evaluación imparcial del modelo. Usar el conjunto de prueba para ajustar parámetros puede sesgar nuestra percepción del rendimiento real del modelo, llevando a una subestimación del error en datos no vistos. Por lo tanto, el conjunto de prueba debe reservarse exclusivamente para una evaluación final del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e2573",
   "metadata": {},
   "source": [
    "### 4) La validación cruzada es un método esencial para estimar el rendimiento de un modelo dividiendo el conjunto de entrenamiento en k subconjuntos distintos, conocidos como \"folds\", particiones o subconjuntos. Cada fold se emplea una vez como conjunto de prueba, mientras que los k-1 folds restantes componen el conjunto de entrenamiento. La elección del valor de k es crucial. Si k es demasiado pequeño, aunque el proceso será computacionalmente menos costoso, el estimado del error tendrá mayor varianza y puede no ser representativo, especialmente en datasets pequeños. Por otro lado, un k grande conduce a una estimación de error más precisa con menos sesgo, pero a costa de un mayor tiempo de cálculo. Además, a medida que k crece, los conjuntos de entrenamiento se asemejan más entre sí, lo que puede resultar en una visión demasiado optimista del rendimiento. En el caso donde k = n (siendo n el número de muestras), denominado \"validación cruzada de eliminación\" o \"leave-one-out cross-validation (LOOCV)\", se estima el modelo n veces. Esta aproximación, aunque precisa, es computacionalmente intensiva, especialmente para conjuntos de datos de gran tamaño o modelos intrincados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
